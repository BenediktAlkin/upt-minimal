{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# setup environment\n",
    "!pip install kappamodules\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "# checkout repo\n",
    "!git clone https://github.com/BenediktAlkin/upt-minimal.git\n",
    "%cd upt-minimal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 CIFAR10 Image Classification\n",
    "\n",
    "At its core, UPT is very similar to a [Vision Tranformer (ViT)](https://arxiv.org/abs/2010.11929).\n",
    "As we omit the latent rollout for this tutorial, UPT is essentially a ViT where the first and last layer are different.\n",
    "The first layer of UPT, a message passing layer to supernodes, is a patch embedding layer in ViT. The last layer\n",
    "of a UPT, a perceiver decoder, is a linear classification head in ViT. There are some more differences, but for\n",
    "the sake of simplicity we'll leave it at that. Changes are highlighted in the orange bounding box.\n",
    "\n",
    "![title](\"schematics/upt_vit.svg\")\n",
    "\n",
    "So this first example converts UPT back into a ViT to get familiar with the model. We do this because ViTs can easily \n",
    "process simple images, such as CIFAR10, which are very easy to load and process. Later parts of the tutorial will \n",
    "gradually move away from simple regular grid data (such as images) to irregular grid data (such as point clouds). But \n",
    "for now, we start with image classification on CIFAR10.\n",
    "\n",
    "Loading data can be done in a few lines of code:"
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a data directory\n",
    "from pathlib import Path\n",
    "data_root = Path(\"./data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "\n",
    "# initialize CIFAR10\n",
    "from torchvision.datasets import CIFAR10\n",
    "train_dataset = CIFAR10(root=data_root, train=True, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize image\n",
    "image, label = train_dataset[0]\n",
    "print(f\"label: {label}\")\n",
    "image.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UPT is designed for irregular grid data, so we make some small adjustments to do image classification:\n",
    "- instead of the supernode pooling (which requires irregular grid data) we use a ViT patch embedding to handle images\n",
    "- instead of the Perceiver Decoder (which decodes the latent space per position), we instead classify the latent space\n",
    "into 10 classes by averaging all latent tokens and then using a linear classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.encoder_image import EncoderImage\n",
    "encoder = EncoderImage(\n",
    "    # CIFAR has 3 channels (RGB)\n",
    "    input_dim=3,\n",
    "    # CIFAR has 32x32 images -> patch_size=4 results in 64 patch tokens\n",
    "    resolution=32,\n",
    "    patch_size=4,\n",
    "    # ViT-T latent dimension\n",
    "    enc_dim=192,\n",
    "    enc_num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    enc_depth=4,\n",
    "    # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "    # perc_dim=dim,\n",
    "    # perc_num_heads=num_heads,\n",
    "    # num_latent_tokens=32,\n",
    ")\n",
    "\n",
    "# we can now encode images\n",
    "image, label = train_dataset[0]\n",
    "# convert image to a tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "tensor = ToTensor()(image)\n",
    "encoded_image = encoder(tensor)\n",
    "print(f\"encoded_image.shape: {encoded_image.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this setting, we dont really need an approximator, but we'll keep it for consistency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.approximator import Approximator\n",
    "approximator = Approximator(\n",
    "    # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "    input_dim=192,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "\n",
    "approximator_output = approximator(encoded_image)\n",
    "print(f\"approximator_output.shape: {approximator_output.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder now only uses some transformers, then averages all tokens and classifies the image with a simple linear\n",
    "head."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.decoder_classifier import DecoderClassifier\n",
    "decoder = DecoderClassifier(\n",
    "    # tell the decoder the dimension of the input (dim of approximator)\n",
    "    input_dim=192,\n",
    "    # CIFAR10 has 10 classes\n",
    "    num_classes=10,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "prediction = decoder(approximator_output)\n",
    "print(f\"prediction.shape: {prediction.shape}\")\n",
    "print(f\"decoder predicted class: {prediction.argmax(dim=1)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a model\n",
    "Now we can put it all together and train an image classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from upt.models.approximator import Approximator\n",
    "from upt.models.decoder_classifier import DecoderClassifier\n",
    "from upt.models.encoder_image import EncoderImage\n",
    "from upt.models.upt_image_classifier import UPTImageClassifier\n",
    "# initialize device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# initialize dataset\n",
    "data_root = Path(\"./data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "transform = ToTensor()\n",
    "train_dataset = CIFAR10(root=data_root, train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root=data_root, train=False, download=True, transform=transform)\n",
    "\n",
    "# hyperparameters\n",
    "dim = 192  # ~6M parameter model\n",
    "num_heads = 3\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# initialize model\n",
    "model = UPTImageClassifier(\n",
    "    encoder=EncoderImage(\n",
    "        # CIFAR has 3 channels (RGB)\n",
    "        input_dim=3,\n",
    "        # CIFAR has 32x32 images -> patch_size=4 results in 64 patch tokens\n",
    "        resolution=32,\n",
    "        patch_size=4,\n",
    "        # ViT-T latent dimension\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        enc_depth=4,\n",
    "        # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "        # perc_dim=dim,\n",
    "        # perc_num_heads=num_heads,\n",
    "        # num_latent_tokens=32,\n",
    "    ),\n",
    "    approximator=Approximator(\n",
    "        # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "        input_dim=dim,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    "    decoder=DecoderClassifier(\n",
    "        # tell the decoder the dimension of the input (dim of approximator)\n",
    "        input_dim=dim,\n",
    "        # CIFAR10 has 10 classes\n",
    "        num_classes=10,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# initialize optimizer and learning rate schedule (linear warmup for first 10% -> linear decay)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "lrs = torch.concat(\n",
    "    [\n",
    "        # linear warmup\n",
    "        torch.linspace(0, optim.defaults[\"lr\"], warmup_updates),\n",
    "        # linear decay\n",
    "        torch.linspace(optim.defaults[\"lr\"], 0, total_updates - warmup_updates),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# train model\n",
    "update = 0\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.update(0)\n",
    "pbar.set_description(\"train_loss: ????? train_accuracy: ????% test_accuracy: ????%\")\n",
    "test_accuracy = 0.0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "loss = None\n",
    "train_accuracy = None\n",
    "for _ in range(epochs):\n",
    "    # train for an epoch\n",
    "    model.train()\n",
    "    for x, y in train_dataloader:\n",
    "        # prepare forward pass\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # schedule learning rate\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lrs[update]\n",
    "\n",
    "        # forward pass\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update step\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # status update\n",
    "        train_accuracy = (y_hat.argmax(dim=1) == y).sum() / y.numel()\n",
    "        update += 1\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.4f} \"\n",
    "            f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "            f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "        )\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # evaluate\n",
    "    num_correct = 0\n",
    "    for x, y in test_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = model(x)\n",
    "        num_correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "    test_accuracy = num_correct / len(test_dataset)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    pbar.set_description(\n",
    "        f\"train_loss: {loss.item():.4f} \"\n",
    "        f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "        f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot the learning curves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].plot(range(len(train_losses)), train_losses)\n",
    "axes[0].set_xlabel(\"Updates\")\n",
    "axes[0].set_ylabel(\"Train Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(range(len(train_accuracies)), train_accuracies)\n",
    "axes[1].set_xlabel(\"Updates\")\n",
    "axes[1].set_ylabel(\"Train Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[2].plot(range(len(test_accuracies)), test_accuracies, marker=\"o\")\n",
    "axes[2].set_xlabel(\"Epochs\")\n",
    "axes[2].set_ylabel(\"Test Accuracy\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not the best classifier, but you know: it works! There are of course a lot of things one could improve in this pipeline\n",
    "but it should show you how UPTs can also process regular grid data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
