{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Preliminaries\n",
    "\n",
    "This notebook will give you an introduction to the basics needed for understanding Universal Physics Transformers (UPT).\n",
    "- Sparse tensors\n",
    "- Architecture Overview\n",
    "- Perceiver Pooling\n",
    "- Perceiver Decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Sparse tensors\n",
    "\n",
    "When handling data from irregular grid, the concept of sparse tensors is commonly used because it allows for efficient\n",
    "computations compared to a dense counterpart.\n",
    "\n",
    "What is a dense tensor? Consider the example of a pointcloud. Lets say we have a set of 5 pointclouds with\n",
    "10, 100, 1000, 10000 and 100000 points respectively (each point has 3 coordinates).\n",
    "In order to train a neural network on it we need to convert it into a pytorch tensor and process multiple pointclouds\n",
    "at once.\n",
    "\n",
    "A dense representation would produce a 3D tensor `(batch_size, max_num_points, 3)` where max_num_points is padded to the\n",
    "largest number of points in a single pointcloud for each batch. This would produce a `(5, 100000, 3)` tensor which consists\n",
    "mostly of padded values (our 5 pointclouds consist of 111.110 points, but the dense representation adds 388.890\n",
    "additional points for padding).\n",
    "\n",
    "A sparse representation would not require this additional overhead as it simply produces a 2D tensor where the\n",
    "first dimension \"squashes\" the `batch_size` and `num_points` dimension into one, i.e. it would produce a\n",
    "tensor of shape (111.110, 3). Much smaller better than the dense representation and no additional padding needed.\n",
    "But which point belongs to which pointcloud? In order to preserve this representation, an additional tensor is created\n",
    "that stores which index of the sparse tensor belongs to which pointcloud. This tensor is commonly called `batch_idx`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create pointclouds\n",
    "import torch\n",
    "point_clouds = [\n",
    "    torch.randn(10, 3),\n",
    "    torch.randn(100, 3),\n",
    "    torch.randn(1000, 3),\n",
    "    torch.randn(10000, 3),\n",
    "    torch.randn(100000, 3),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a dense tensor representing of all point clouds\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "dense = pad_sequence(point_clouds)\n",
    "print(f\"dense.shape: {dense.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a sparse tensor representation of all point clouds\n",
    "sparse = torch.concat(point_clouds)\n",
    "print(f\"sparse.shape: {sparse.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create batch_idx tensor to assign indices of the sparse tensor to indices of the pointcloud\n",
    "batch_idx = torch.tensor([[i] * len(point_cloud) for i, point_cloud in enumerate(point_clouds)])\n",
    "print(f\"batch_idx.shape: {batch_idx.shape}\")\n",
    "print(f\"the first 10 samples belong to the first pointcloud (i.e. point_clouds[0]): {batch_idx[:10]}\")\n",
    "print(f\"the 11th point in the sparse tensor belongs to the second pointcloud (i.e. point_clouds[1]): {batch_idx[10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dense tensors are very nice for things like regular grid data (images, videos, ...) but for data from irregular grids\n",
    "sparse tensors are needed for efficient processing.\n",
    "\n",
    "In UPTs, the input and output are both sparse tensors but within the model, the tensors are converted from sparse\n",
    "to dense and then back to sparse. That is because message passing layers use a sparse representation and \n",
    "transformers use a dense representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "0.1 Architecture Overview\n",
    "\n",
    "First, lets have a more detailed look at the architecture. In this tutorial, we will not consider inverse\n",
    "encoding/decoding (to enable the latent rollout) as it adds a lot of complexity to the model.\n",
    "\n",
    "We have a simple transformer based model which we conceptually split into 3 components: encoder, approximator and decoder.\n",
    "\n",
    "The **encoder** processes input features (e.g. velocities, pressure, ...) and input positions at timestep $t$ and\n",
    "encodes it into a latent representation $\\text{latent}_t$. Input features and input positions\n",
    "\n",
    "![title](\"schematics/architecture.svg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.1 Perceiver Pooling\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}