{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# setup\n",
    "!pip install kappamodules\n",
    "!git clone git@github.com:BenediktAlkin/upt-minimal.git\n",
    "!pip install torch_geometric\n",
    "!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Preliminaries\n",
    "\n",
    "This notebook will give you an introduction to the basics needed for understanding Universal Physics Transformers (UPT).\n",
    "- Sparse tensors\n",
    "- Positional Encoding\n",
    "- Architecture Overview\n",
    "  - Encoder\n",
    "  - Approximator\n",
    "  - Decoder\n",
    "\n",
    "We recommend to familiarize yourself with the following papers if you haven't already:\n",
    "- [Transformer](https://arxiv.org/abs/1706.03762)\n",
    "- [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "- [Perceiver IO](https://arxiv.org/abs/2107.14795)\n",
    "\n",
    "\n",
    "The following notebooks will start from scratch with a simple image classification example. Then, each consecutive notebook will introduce a new component to the pipeline:\n",
    "- [CIFAR10 image classification](https://github.com/BenediktAlkin/upt-minimal/blob/main/2_image_classification.ipynb): start from a basic example (regular grid input, scalar output, easy encoder, simple classification decoder)\n",
    "- [CIFAR10 autoencoder](https://github.com/BenediktAlkin/upt-minimal/blob/main/3_image_autoencoder.ipynb): introduce the perceiver decoder to query at arbitrary positions\n",
    "- [SparseCIFAR10 image classification](https://github.com/BenediktAlkin/upt-minimal/blob/main/4_pointcloud_classification.ipynb): introduce handling point clouds via sparse tensors and supernode message passing\n",
    "- [SparseCIFAR10 autoencoder](https://github.com/BenediktAlkin/upt-minimal/blob/main/5_pointcloud_autoencoder.ipynb): introduce handling point clouds as output\n",
    "- [TODO](https://github.com/BenediktAlkin/upt-minimal/blob/main/TODO.ipynb): train on a single trajectory of our transient flow simulations\n",
    " \n",
    "\n",
    "As data comes in all shapes and forms, this gradual introduction of components will hopefully give you a more complete overview of how to apply the UPT framework to various tasks.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Sparse tensors\n",
    "\n",
    "When handling data from irregular grid, the concept of sparse tensors is commonly used because it allows for efficient computations compared to a dense counterpart.\n",
    "\n",
    "What is a dense tensor? Consider the example of a pointcloud. Lets say we have a set of 5 pointclouds with 10, 100, 1000, 10000 and 100000 points respectively (each point has 3 coordinates). In order to train a neural network on it we need to convert it into a pytorch tensor and process multiple pointclouds at once.\n",
    "\n",
    "A dense representation would produce a 3D tensor `(batch_size, max_num_points, 3)` where max_num_points is padded to the largest number of points in a single pointcloud for each batch. This would produce a `(5, 100000, 3)` tensor which consists mostly of padded values (our 5 pointclouds consist of 111.110 points, but the dense representation adds 388.890 additional points for padding)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create pointclouds\n",
    "import torch\n",
    "point_clouds = [\n",
    "    torch.randn(10, 3),\n",
    "    torch.randn(100, 3),\n",
    "    torch.randn(1000, 3),\n",
    "    torch.randn(10000, 3),\n",
    "    torch.randn(100000, 3),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a dense tensor representing of all point clouds\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "dense = pad_sequence(point_clouds, batch_first=True)\n",
    "print(f\"dense.shape: {dense.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a sparse tensor representation of all point clouds\n",
    "sparse = torch.concat(point_clouds)\n",
    "print(f\"sparse.shape: {sparse.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A sparse representation does not require this additional overhead as it simply produces a 2D tensor where the first dimension \"squashes\" the `batch_size` and `num_points` dimension into one, i.e. it would produce a tensor of shape (111.110, 3). Much smaller better than the dense representation and no additional padding needed. But which point belongs to which pointcloud? In order to preserve this representation, an additional tensor is created that stores which index of the sparse tensor belongs to which pointcloud. This tensor is commonly called `batch_idx`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create batch_idx tensor to assign indices of the sparse tensor to indices of the pointcloud\n",
    "batch_idx = torch.tensor([[i] * len(point_cloud) for i, point_cloud in enumerate(point_clouds)])\n",
    "print(f\"batch_idx.shape: {batch_idx.shape}\")\n",
    "print(f\"the first 10 samples belong to the first pointcloud (i.e. point_clouds[0]): {batch_idx[:10]}\")\n",
    "print(f\"the 11th point in the sparse tensor belongs to the second pointcloud (i.e. point_clouds[1]): {batch_idx[10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dense tensors are very nice for things like regular grid data (images, videos, ...) but for data from irregular grids, sparse tensors are needed for efficient processing.\n",
    "\n",
    "In UPTs, the input and output are both sparse tensors but within the model, the tensors are converted from sparse to dense and then back to sparse. That is because message passing layers use a sparse representation and transformers use a dense representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Positional Encoding\n",
    "\n",
    "Positional encodings are a good way to encode 1D, 2D or 3D positional vectors for neural networks to process them. They are commonly used in transformers and make use of sine and cosine waves of different frequencies.\n",
    "\n",
    "We use the the positional embedding from the original [transformer](https://arxiv.org/abs/1706.03762) paper, which was designed for language modeling, i.e. 1D positions where the smallest position is 0 and the largest one depends on the context length and is typically a couple of thousands. In computer vision, this 1D position is used twice, once to encode the x position and once to encode the y position where a common gridsize for [Vision Transformers (ViTs)](https://arxiv.org/abs/2010.11929) is 14x14, so 196 tokens in total. We use the positional encoding of ViTs, so we encode each dimension seperately and concatenate them. For example, a 3D position would encode x, y and z positions seperately with a third of the latent dimension and then concatenate the three encodings. Additionally, as positions in physical systems can be negative, we rescale all positions via min/max normalization to range from 0 to 200.\n",
    "\n",
    "How does such a positional encoding look? Lets plot it:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from kappamodules.layers.continuous_sincos_embed import ContinuousSincosEmbed\n",
    "def plot(ndim):\n",
    "    # positions in range [0, 200]\n",
    "    pos = torch.linspace(0, 200, 1001)\n",
    "    # the embeddings have dimension 192\n",
    "    embed = ContinuousSincosEmbed(dim=192, ndim=ndim)\n",
    "    # embed the positions\n",
    "    posembed = embed(pos.unsqueeze(0).unsqueeze(2)).squeeze(2).squeeze(0)\n",
    "    # plot it\n",
    "    plt.imshow(posembed)\n",
    "    plt.xlabel(\"dim\")\n",
    "    plt.ylabel(\"position\")\n",
    "    plt.yticks(ticks=[0, len(x) - 1], labels=[f\"{min_value}\", f\"{max_value}\"])\n",
    "    plt.title(f\"range=[{min_value}, {max_value}]\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1D positional embeddings\n",
    "plot(ndim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 2D positional embeddings\n",
    "plot(ndim=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 3D positional embeddings\n",
    "plot(ndim=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Architecture Overview\n",
    "\n",
    "First, lets have a more detailed look at the architecture. In this tutorial, we will not consider inverse encoding/decoding (to enable the latent rollout) as it adds a lot of complexity to the model. Conceptually, the seperation into encoder/approximator/decoder is only relevant when using a latent rollout, but we stick to this seperation for consistency.\n",
    "\n",
    "We have a simple transformer based model which we conceptually split into 3 components: encoder, approximator and decoder. We use transformer blocks as the main component in our model (e.g. 12 transformer blocks) which we split evenly across encoder, approximator and decoder.\n",
    "\n",
    "![title](\"schematics/architecture.svg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Encoder\n",
    "\n",
    "The **encoder** processes input features (e.g. velocities, pressure, ...) and input positions at timestep $t$ and\n",
    "encodes it into a latent representation $\\text{latent}_t$. Input features and input positions are sparse tensors.\n",
    "The input features are first processed with a shallow MLP. Then, the input positions are added to the result of the\n",
    "MLP. This representation is then used for a message passing, where messages are only passed to selected supernodes.\n",
    "We randomly select a fixed number nodes from each pointcloud during dataloading which are then used as \"supernodes\".\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create some input features and positions\n",
    "import torch\n",
    "# create 16 points with 2 features each\n",
    "input_feat = torch.randn(16, 2)\n",
    "# 3D coordinates (scaled to be in [0, 200])\n",
    "input_pos = torch.rand(16, 3) * 200\n",
    "# create batch_idx (we assume 2 point clouds with length 6 and 10)\n",
    "batch_idx = torch.tensor([0] * 6 + [1] * 10)\n",
    "# select 2 supernodes per pointcloud\n",
    "supernode_idxs = torch.tensor([0, 2, 9, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.modules.supernode_pooling import SupernodePooling\n",
    "supernode_pooling = SupernodePooling(\n",
    "    # use a large radius because we dont have a lot of points\n",
    "    radius=100,\n",
    "    # max_degree is not relevant here because we have too little points\n",
    "    max_degree=32,\n",
    "    # same as dimension of input_feat\n",
    "    input_dim=2,\n",
    "    # we use a small hidden dimension for the MLP here\n",
    "    hidden_dim=8,\n",
    "    # same as dimension of input_pos\n",
    "    ndim=3,\n",
    ")\n",
    "\n",
    "supernodes = supernode_pooling(\n",
    "    input_feat=input_feat,\n",
    "    input_pos=input_pos,\n",
    "    supernode_idxs=supernode_idxs,\n",
    "    batch_idx=batch_idx,\n",
    ")\n",
    "print(f\"supernodes.shape: {supernodes.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the number of supernodes is fixed per pointcloud, the supernode pooling can seamlessly output a dense tensor which is then used for processing the supernodes further with some transformer blocks (we use a pre-norm architecture)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create 4 transformer blocks\n",
    "from kappamodules.transformer import PrenormBlock\n",
    "blocks = [PrenormBlock(dim=8, num_heads=2) for _ in range(4)]\n",
    "\n",
    "# process supernodes with transformer\n",
    "transformed_supernodes = supernodes\n",
    "for block in blocks:\n",
    "    transformed_supernodes = block(transformed_supernodes)\n",
    "print(f\"transformed_supernodes.shape: {transformed_supernodes.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally, a \"Perceiver Pooling\" layer can be applied as last layer of the encoder. This layer reduces the number of supernodes into a fixed number of latent tokens. If the \"Perceiver Pooling\" is not used, the number of latent tokens is equivalent to the number of supernodes.\n",
    "![title](\"schematics/perceiver_pooling.svg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create perceiver block\n",
    "from kappamodules.transformer import PerceiverBlock\n",
    "# create query (this is a learnable vector later on)\n",
    "# we use 2 latent tokens here\n",
    "query = torch.randn(2, 8)\n",
    "encoder_perceiver = PerceiverBlock(dim=8, num_heads=2)\n",
    "latent_tokens = encoder_perceiver(q=query, kv=transformed_supernodes)\n",
    "print(f\"latent_tokens.shape: {latent_tokens.shape}\")\n",
    "\n",
    "# the same thing can be done in one go\n",
    "from kappamodules.transformer import PerceiverPoolingBlock\n",
    "encoder_perceiver_onego = PerceiverPoolingBlock(num_query_tokens=2, dim=8, num_heads=2)\n",
    "latent_tokens_onego = encoder_perceiver_onego(kv=transformed_supernodes)\n",
    "print(f\"latent_tokens_onego.shape: {latent_tokens_onego.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Approximator\n",
    "\n",
    "The approximator takes the `latent_tokens` and pushes them forward by one timestep. It simply consists of some transformer blocks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kappamodules.transformer import PrenormBlock\n",
    "approximator = nn.Sequential(*[PrenormBlock(dim=8, num_heads=2) for _ in range(4)])\n",
    "latent_tokens_next_timestep = approximator(latent_tokens)\n",
    "print(f\"latent_tokens_next_timestep.shape: {latent_tokens_next_timestep.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Decoder\n",
    "\n",
    "The decoder takes the latent_tokens and decodes them into the original space of the input data. It does this by querying the latent space at arbitrary positions. It first employs some transformer blocks, followed by a perceiver decoder block. The output positions are encoded via a shallow MLP before being used as query vector for the perceiver. ![title](\"schematics/perceiver_decoder.svg\")\n",
    "\n",
    "For training, the output positions need to have an associated ground truth value in the dataset as the model is trained via an mean-squared-error loss between the predictions at the output positions and the ground truth value at the output positions. For inference, the output positions can be arbitrary. Also: output positions and input positions do not have to match (also not during training)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create some output positions (we query at 4 positions per pointcloud)\n",
    "import torch\n",
    "# (2=number of pointclouds, 4=number of output positions, 3=3D positions)\n",
    "output_pos = torch.randn(2, 4, 3)\n",
    "\n",
    "# some transformer blocks\n",
    "from kappamodules.transformer import PrenormBlock\n",
    "decoder_transformer = nn.Sequential(*[PrenormBlock(dim=8, num_heads=2) for _ in range(4)])\n",
    "\n",
    "# create perceiver\n",
    "from kappamodules.transformer import PerceiverBlock\n",
    "decoder_perceiver = PerceiverBlock(dim=8, num_heads=2)\n",
    "\n",
    "# create positional encoding and MLP to encode positions\n",
    "from kappamodules.layers import ContinuousSincosEmbed\n",
    "pos_embed = ContinuousSincosEmbed(dim=8, ndim=3)\n",
    "output_pos_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8, 8),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# apply transformer\n",
    "decoder_perceiver_kv = decoder_transformer(latent_tokens_next_timestep)\n",
    "\n",
    "# encode output positions into query vector\n",
    "query = output_pos_mlp(pos_embed(output_pos))\n",
    "\n",
    "# apply perceiver decoder\n",
    "pred = decoder_perceiver(q=query, kv=decoder_perceiver_kv)\n",
    "\n",
    "print(f\"decoder_perceiver_kv.shape: {decoder_perceiver_kv.shape}\")\n",
    "print(f\"query.shape: {query.shape}\")\n",
    "print(f\"pred.shape: {pred.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
