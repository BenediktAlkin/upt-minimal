{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# setup environment\n",
    "!pip install kappamodules\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(device)\n",
    "\n",
    "# checkout repo\n",
    "!git clone https://github.com/BenediktAlkin/upt-minimal.git\n",
    "%cd upt-minimal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 CIFAR10 Autoencoder\n",
    "\n",
    "Now that we know how the core of UPT works, its time to introduce some of the new components.\n",
    "In this notebook, we will change the last layer back to what UPT uses, i.e. a perceiver decoder.\n",
    "To showcase this, we'll train a simple autoencoder by decoding the latent space of the model\n",
    "via regular grid positions, i.e. we query the latent space with the positions of each pixel\n",
    "to reconstruct the image from the latent space.\n",
    "\n",
    "We'll keep the ViT patch embedding as first layer as we are working with images as input but\n",
    "change the decoder back to the perceiver decoder.\n",
    "\n",
    "![title](\"schematics/upt_dense_autoencoder.svg\")"
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same dataloading as before except that we don't need the label here, because we train an autoencoder. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# make a data directory\n",
    "from pathlib import Path\n",
    "data_root = Path(\"./data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "\n",
    "# initialize CIFAR10\n",
    "from torchvision.datasets import CIFAR10\n",
    "train_dataset = CIFAR10(root=data_root, train=True, download=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# visualize image\n",
    "image, label = train_dataset[0]\n",
    "print(f\"label: {label}\")\n",
    "image.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The encoder is the same as in the previous tutorial (UPT with a ViT patch embedding instead of\n",
    "the message passing to supernodes)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.models.encoder_image import EncoderImage\n",
    "encoder = EncoderImage(\n",
    "    # CIFAR has 3 channels (RGB)\n",
    "    input_dim=3,\n",
    "    # CIFAR has 32x32 images -> patch_size=4 results in 64 patch tokens\n",
    "    resolution=32,\n",
    "    patch_size=4,\n",
    "    # ViT-T latent dimension\n",
    "    enc_dim=192,\n",
    "    enc_num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    enc_depth=4,\n",
    "    # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "    # perc_dim=dim,\n",
    "    # perc_num_heads=num_heads,\n",
    "    # num_latent_tokens=32,\n",
    ")\n",
    "\n",
    "# we can now encode images\n",
    "image, label = train_dataset[0]\n",
    "# convert image to a tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "tensor = ToTensor()(image)\n",
    "encoded_image = encoder(tensor)\n",
    "print(f\"encoded_image.shape: {encoded_image.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this setting, we dont really need an approximator, but we'll keep it for consistency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.models.approximator import Approximator\n",
    "approximator = Approximator(\n",
    "    # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "    input_dim=192,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "\n",
    "approximator_output = approximator(encoded_image)\n",
    "print(f\"approximator_output.shape: {approximator_output.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now need a decoder that can predict images, so we use the perceiver decoder and query\n",
    "the latent space at each pixel position to produce an image."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.models.decoder_perceiver import DecoderPerceiver\n",
    "decoder = DecoderPerceiver(\n",
    "    # tell the decoder the dimension of the input (dim of approximator)\n",
    "    input_dim=192,\n",
    "    # 3 channels for RGB\n",
    "    output_dim=3,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# to query the latent space, we need pixel positions (CIFAR10 has 32x32 pixels)\n",
    "import einops\n",
    "import torch\n",
    "output_pos = einops.rearrange(\n",
    "    torch.stack(torch.meshgrid([torch.arange(32), torch.arange(32)], indexing=\"ij\")),\n",
    "    \"ndim height width -> (height width) ndim\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# these positions are the pixel positions traversed row-wise from top-left\n",
    "print(f\"output_pos[0]: {output_pos[0]}\")\n",
    "print(f\"output_pos[1]: {output_pos[1]}\")\n",
    "print(f\"output_pos[2]: {output_pos[2]}\")\n",
    "print(f\"output_pos[32]: {output_pos[32]}\")\n",
    "print(f\"output_pos[1023]: {output_pos[1023]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets decode an image!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prediction = decoder(approximator_output, output_pos=output_pos)\n",
    "print(f\"prediction.shape: {prediction.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cool thing is, we are not bound to pixel positions, so we could decode more pixels\n",
    "than in the input. So lets decode a 64x64 image!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create output_pos for 64x64 image\n",
    "output_pos_double = einops.rearrange(\n",
    "    torch.stack(torch.meshgrid([torch.arange(64), torch.arange(64)], indexing=\"ij\")),\n",
    "    \"ndim height width -> (height width) ndim\",\n",
    ")\n",
    "# if we'd actually train a model, we'd need to normalize the positions such that \n",
    "# the max value of the double resolution positions is equal to the max value\n",
    "# of the 32x32 positions. So we'd not count pixel positions as 0, 1, 2, ... but\n",
    "# rather 0, 0.5, 1, 1.5.\n",
    "print(f\"output_pos_double.max before renormalization: {output_pos_double.max()}\")\n",
    "output_pos_double = output_pos_double / 63 * 31\n",
    "print(f\"output_pos_double.max after renormalization: {output_pos_double.max()}\")\n",
    "print(f\"output_pos.max: {output_pos.max()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# decode 64x64 image\n",
    "double_resolution_pred = decoder(approximator_output, output_pos=output_pos_double)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a model\n",
    "Now lets train the autoencoder."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from upt.models.approximator import Approximator\n",
    "from upt.models.decoder_perceiver import DecoderPerceiver\n",
    "from upt.models.encoder_image import EncoderImage\n",
    "from upt.models.upt_image_autoencoder import UPTImageAutoencoder\n",
    "\n",
    "# initialize device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# initialize dataset\n",
    "data_root = Path(\"./data\")\n",
    "data_root.mkdir(exist_ok=True)\n",
    "transform = ToTensor()\n",
    "train_dataset = CIFAR10(root=data_root, train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root=data_root, train=False, download=True, transform=transform)\n",
    "\n",
    "# hyperparameters\n",
    "dim = 192  # ~6M parameter model\n",
    "num_heads = 3\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# initialize model\n",
    "model = UPTImageAutoencoder(\n",
    "    encoder=EncoderImage(\n",
    "        # CIFAR has 3 channels (RGB)\n",
    "        input_dim=3,\n",
    "        # CIFAR has 32x32 images -> patch_size=4 results in 64 patch tokens\n",
    "        resolution=32,\n",
    "        patch_size=4,\n",
    "        # ViT-T latent dimension\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        enc_depth=4,\n",
    "        # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "        # perc_dim=dim,\n",
    "        # perc_num_heads=num_heads,\n",
    "        # num_latent_tokens=32,\n",
    "    ),\n",
    "    approximator=Approximator(\n",
    "        # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "        input_dim=dim,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    "    decoder=DecoderPerceiver(\n",
    "        # tell the decoder the dimension of the input (dim of approximator)\n",
    "        input_dim=dim,\n",
    "        # 3 channels for RGB\n",
    "        output_dim=3,\n",
    "        # images have 2D coordinates\n",
    "        ndim=2,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "        # reshape to image after decoding\n",
    "        unbatch_mode=\"image\",\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# initialize optimizer and learning rate schedule (linear warmup for first 10% -> linear decay)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "lrs = torch.concat(\n",
    "    [\n",
    "        # linear warmup\n",
    "        torch.linspace(0, optim.defaults[\"lr\"], warmup_updates),\n",
    "        # linear decay\n",
    "        torch.linspace(optim.defaults[\"lr\"], 0, total_updates - warmup_updates),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# output positions are fixed for training, we query on a regular grid\n",
    "# CIFAR has 32x32 pixels\n",
    "# output_pos will be a tensor of shape (32 * 32, 2) with and will contain x and y indices\n",
    "# output_pos[0] = [0, 0]\n",
    "# output_pos[1] = [0, 1]\n",
    "# output_pos[2] = [0, 2]\n",
    "# ...\n",
    "# output_pos[32] = [1, 0]\n",
    "# output_pos[1024] = [31, 31]\n",
    "output_pos = einops.rearrange(\n",
    "    torch.stack(torch.meshgrid([torch.arange(32), torch.arange(32)], indexing=\"ij\")),\n",
    "    \"ndim height width -> (height width) ndim\",\n",
    ")\n",
    "output_pos = output_pos.to(device)\n",
    "# convert output_pos from [0, 31] to [0, 1000] for better behavior with sin-cos pos embeddings\n",
    "output_pos = output_pos / 31 * 1000\n",
    "# decoder needs float dtype\n",
    "output_pos = output_pos.float()\n",
    "\n",
    "# train model\n",
    "update = 0\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.update(0)\n",
    "pbar.set_description(\"train_loss: ????? test_loss. ?????\")\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_loss = float(\"inf\")\n",
    "loss = None\n",
    "for _ in range(epochs):\n",
    "    # train for an epoch\n",
    "    model.train()\n",
    "    for x, _ in train_dataloader:\n",
    "        # prepare forward pass\n",
    "        x = x.to(device)\n",
    "\n",
    "        # schedule learning rate\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lrs[update]\n",
    "\n",
    "        # forward pass\n",
    "        x_hat = model(x, output_pos=einops.repeat(output_pos, \"... -> bs ...\", bs=len(x)))\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update step\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # status update\n",
    "        update += 1\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.4f} \"\n",
    "            f\"test_loss: {test_loss:.4f} \"\n",
    "        )\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # evaluate\n",
    "    test_loss = 0.\n",
    "    for x, _ in test_dataloader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        test_loss += F.mse_loss(x_hat, x, reduction=\"none\").flatten(start_dim=1).mean(dim=1).sum().item()\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    pbar.set_description(\n",
    "        f\"train_loss: {loss.item():.4f} \"\n",
    "        f\"test_loss: {test_loss:.4f} \"\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot the learning curves!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(range(len(train_losses)), train_losses)\n",
    "axes[0].set_xlabel(\"Updates\")\n",
    "axes[0].set_ylabel(\"Train Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(range(len(test_losses)), test_losses)\n",
    "axes[1].set_xlabel(\"Updates\")\n",
    "axes[1].set_ylabel(\"Test Loss\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not the best classifier, but you know: it works! There are of course a lot of things one could improve in this pipeline\n",
    "but it should show you how UPTs can also process regular grid data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
