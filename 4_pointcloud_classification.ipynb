{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# setup environment\n",
    "!pip install kappamodules\n",
    "!pip install torch_geometric\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(device)\n",
    "# might need to torch version to the one installed in colab\n",
    "!pip install torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "\n",
    "# checkout repo\n",
    "!git clone https://github.com/BenediktAlkin/upt-minimal.git\n",
    "%cd upt-minimal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Pointcloud Classification\n",
    "\n",
    "Now we know how the model and the decoder works, but we've only trained with images (regular grid data).\n",
    "We'll want to train UPT on irregular grid data, so lets do it. First, we need a dataset but since we\n",
    "are already familiar with CIFAR10, we'll simply treat it as a pointcloud. So every pixel is a point in 2D\n",
    "space that has an RGB value. So essentially, we convert CIFAR10 into a sparse tensor representation\n",
    "which we'll call `SparseCIFAR10`.\n",
    "\n",
    "To illustrate the sparse representation better, we'll simply drop some pixels in the input (which we\n",
    "can now easily do because the encoder with a supernode pooling as initial layer can handle arbitrary\n",
    "pointclouds).\n",
    "\n",
    "![title](\"schematics/upt_sparse_autoencoder.svg\")\n",
    "\n",
    "Now dataloading gets a bit trickier, because we need supernodes and we need to create a sparse tensor \n",
    "via the `collate_fn` of the pytorch [DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "We'll do this via an object, which we'll call `SparseImageClassifierCollator`.\n",
    "\n",
    "To start, lets initialize the dataset and visualize a sample:"
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize CIFAR10\n",
    "from upt.datasets.sparse_cifar10_classifier_dataset import SparseCIFAR10ClassifierDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_dataset = SparseCIFAR10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    # use half of the inputs for training (32x32 pixels = 1024)\n",
    "    num_inputs=512,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now plotting is a bit trickier because we have a sparse tensor\n",
    "# we'll make a scatter plot where each point is colored with the \n",
    "# RGB value of the pixel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = train_dataset[0]\n",
    "x, y = sample[\"input_pos\"].unbind(1)\n",
    "c = sample[\"input_feat\"]\n",
    "plt.scatter(x, y, c=c, marker=\"s\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets see how a forward pass looks like for a single sample (so we don't need a collator yet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder = EncoderSupernodes(\n",
    "    # CIFAR has 3 channels (RGB)\n",
    "    input_dim=3,\n",
    "    # CIFAR is an image dataset -> 2D\n",
    "    ndim=2,\n",
    "    # there are 32x32 pixels so positions are in [0, 31], to have roughly the same input as a ViT\n",
    "    # with patch_size=4, we'll use radius slighly larger than 4\n",
    "    radius=5,\n",
    "    # if we split a 32x32 image into 8x8 gridpoints, each point would cover 4x4 pixels, i.e. 16 pixels (=nodes)\n",
    "    # since we sample supernodes randomly and use a larger radius, it can happen that more than 16 nodes\n",
    "    # are in the radius of a supernode, so we'll use at maximum 32 connections to each supernode\n",
    "    max_degree=32,\n",
    "    # dimension for the supernode pooling -> use same as ViT-T latent dim\n",
    "    gnn_dim=192,\n",
    "    # ViT-T latent dimension\n",
    "    enc_dim=192,\n",
    "    enc_num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    enc_depth=4,\n",
    "    # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "    # perc_dim=dim,\n",
    "    # perc_num_heads=num_heads,\n",
    "    # num_latent_tokens=32,\n",
    ")\n",
    "\n",
    "# for now, we only encode 1 sample, so we need to generate some supernode indices\n",
    "# later the collator will take care of this\n",
    "sample = train_dataset[0]\n",
    "# select 64 random pixels as supernodes\n",
    "supernode_idxs = torch.randperm(len(sample[\"input_feat\"]))[:64]\n",
    "# and we need a batch_idx tensor\n",
    "batch_idx = torch.zeros(len(sample[\"input_feat\"]), dtype=torch.long)\n",
    "encoded_pointcloud = encoder(\n",
    "    input_feat=sample[\"input_feat\"],\n",
    "    input_pos=sample[\"input_pos\"],\n",
    "    supernode_idxs=supernode_idxs,\n",
    "    batch_idx=batch[\"batch_idx\"],\n",
    ")\n",
    "print(f\"encoded_pointcloud.shape: {encoded_pointcloud.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this setting, we dont really need an approximator, but we'll keep it for consistency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.models.approximator import Approximator\n",
    "\n",
    "approximator = Approximator(\n",
    "    # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "    input_dim=192,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "\n",
    "approximator_output = approximator(encoded_pointcloud)\n",
    "print(f\"approximator_output.shape: {approximator_output.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder now only uses some transformers, then averages all tokens and classifies the image with a simple linear\n",
    "head."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from upt.models.decoder_classifier import DecoderClassifier\n",
    "\n",
    "decoder = DecoderClassifier(\n",
    "    # tell the decoder the dimension of the input (dim of approximator)\n",
    "    input_dim=192,\n",
    "    # CIFAR10 has 10 classes\n",
    "    num_classes=10,\n",
    "    # as in ViT-T\n",
    "    dim=192,\n",
    "    num_heads=3,\n",
    "    # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "    depth=4,\n",
    ")\n",
    "prediction = decoder(approximator_output)\n",
    "print(f\"prediction.shape: {prediction.shape}\")\n",
    "print(f\"decoder predicted class: {prediction.argmax(dim=1)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a model\n",
    "Now we can put it all together and train an image classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from upt.models.approximator import Approximator\n",
    "from upt.models.decoder_classifier import DecoderClassifier\n",
    "from upt.models.encoder_supernodes import EncoderSupernodes\n",
    "from upt.models.upt_sparseimage_classifier import UPTSparseImageClassifier\n",
    "from upt.datasets.sparse_cifar10_classifier_dataset import SparseCifar10ClassifierDataset\n",
    "from upt.collators.sparseimage_classifier_collator import SparseImageClassifierCollator\n",
    "\n",
    "# initialize device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# initialize dataset\n",
    "transform = ToTensor()\n",
    "train_dataset = SparseCifar10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # use half of the inputs for training (32x32 pixels = 1024)\n",
    "    num_inputs=512,\n",
    ")\n",
    "test_dataset = SparseCifar10ClassifierDataset(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    # use all inputs for evaluation (32x32 pixels = 1024)\n",
    "    num_inputs=1024,\n",
    ")\n",
    "\n",
    "# hyperparameters\n",
    "dim = 192  # ~6M parameter model\n",
    "num_heads = 3\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# initialize model\n",
    "model = UPTSparseImageClassifier(\n",
    "    encoder=EncoderSupernodes(\n",
    "        # CIFAR has 3 channels (RGB)\n",
    "        input_dim=3,\n",
    "        # CIFAR is an image dataset -> 2D\n",
    "        ndim=2,\n",
    "        # there are 32x32 pixels so positions are in [0, 31], to have roughly the same input as a ViT\n",
    "        # with patch_size=4, we'll use radius slighly larger than 4\n",
    "        radius=5,\n",
    "        # if we split a 32x32 image into 8x8 gridpoints, each point would cover 4x4 pixels, i.e. 16 pixels (=nodes)\n",
    "        # since we sample supernodes randomly and use a larger radius, it can happen that more than 16 nodes\n",
    "        # are in the radius of a supernode, so we'll use at maximum 32 connections to each supernode\n",
    "        max_degree=32,\n",
    "        # dimension for the supernode pooling -> use same as ViT-T latent dim\n",
    "        gnn_dim=dim,\n",
    "        # ViT-T latent dimension\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        enc_depth=4,\n",
    "        # the perceiver is optional, it changes the size of the latent space to NUM_LATENT_TOKENS tokens\n",
    "        # perc_dim=dim,\n",
    "        # perc_num_heads=num_heads,\n",
    "        # num_latent_tokens=32,\n",
    "    ),\n",
    "    approximator=Approximator(\n",
    "        # tell the approximator the dimension of the input (perc_dim or enc_dim of encoder)\n",
    "        input_dim=dim,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    "    decoder=DecoderClassifier(\n",
    "        # tell the decoder the dimension of the input (dim of approximator)\n",
    "        input_dim=dim,\n",
    "        # CIFAR10 has 10 classes\n",
    "        num_classes=10,\n",
    "        # as in ViT-T\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "# setup dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=SparseImageClassifierCollator(num_supernodes=64, deterministic=False),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=SparseImageClassifierCollator(num_supernodes=64, deterministic=True),\n",
    ")\n",
    "\n",
    "# initialize optimizer and learning rate schedule (linear warmup for first 10% -> linear decay)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "lrs = torch.concat(\n",
    "    [\n",
    "        # linear warmup\n",
    "        torch.linspace(0, optim.defaults[\"lr\"], warmup_updates),\n",
    "        # linear decay\n",
    "        torch.linspace(optim.defaults[\"lr\"], 0, total_updates - warmup_updates),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# train model\n",
    "update = 0\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.update(0)\n",
    "pbar.set_description(\"train_loss: ????? train_accuracy: ????% test_accuracy: ????%\")\n",
    "test_accuracy = 0.0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "loss = None\n",
    "train_accuracy = None\n",
    "for _ in range(epochs):\n",
    "    # train for an epoch\n",
    "    for batch in train_dataloader:\n",
    "        # schedule learning rate\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lrs[update]\n",
    "\n",
    "        # forward pass\n",
    "        y_hat = model(\n",
    "            input_feat=batch[\"input_feat\"].to(device),\n",
    "            input_pos=batch[\"input_pos\"].to(device),\n",
    "            supernode_idxs=batch[\"supernode_idxs\"].to(device),\n",
    "            batch_idx=batch[\"batch_idx\"].to(device),\n",
    "        )\n",
    "        y = batch[\"target_class\"].to(device)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update step\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # status update\n",
    "        train_accuracy = (y_hat.argmax(dim=1) == y).sum() / y.numel()\n",
    "        update += 1\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.4f} \"\n",
    "            f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "            f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "        )\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # evaluate\n",
    "    num_correct = 0\n",
    "    for batch in test_dataloader:\n",
    "        y_hat = model(\n",
    "            input_feat=batch[\"input_feat\"].to(device),\n",
    "            input_pos=batch[\"input_pos\"].to(device),\n",
    "            supernode_idxs=batch[\"supernode_idxs\"].to(device),\n",
    "            batch_idx=batch[\"batch_idx\"].to(device),\n",
    "        )\n",
    "        y = batch[\"target_class\"].to(device)\n",
    "        num_correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "    test_accuracy = num_correct / len(test_dataset)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    pbar.set_description(\n",
    "        f\"train_loss: {loss.item():.4f} \"\n",
    "        f\"train_accuracy: {train_accuracy * 100:4.1f}% \"\n",
    "        f\"test_accuracy: {test_accuracy * 100:4.1f}%\"\n",
    "    )\n",
    "pbar.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot the learning curves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].plot(range(len(train_losses)), train_losses)\n",
    "axes[0].set_xlabel(\"Updates\")\n",
    "axes[0].set_ylabel(\"Train Loss\")\n",
    "axes[0].set_title(\"Train Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(range(len(train_accuracies)), train_accuracies)\n",
    "axes[1].set_xlabel(\"Updates\")\n",
    "axes[1].set_ylabel(\"Train Accuracy\")\n",
    "axes[1].set_title(\"Train Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[2].plot(range(len(test_accuracies)), test_accuracies, marker=\"o\")\n",
    "axes[2].set_xlabel(\"Epochs\")\n",
    "axes[2].set_ylabel(\"Test Accuracy\")\n",
    "axes[2].set_title(\"Test Accuracy\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can also classify pointclouds with UPT."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}